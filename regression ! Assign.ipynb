{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd57203-3a2e-47de-a306-60820b4e20e2",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b4095-b9c7-40c0-a2f5-a148d1550c91",
   "metadata": {},
   "source": [
    "ANS:-\n",
    "Simple Linear Regression\n",
    "Definition:\n",
    "Simple linear regression is a statistical method used to model the relationship between a single independent variable (predictor) and a dependent variable (response). The relationship is modeled using a straight line, which is defined by the equation:\n",
    "\n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+ϵ\n",
    "\n",
    "where:\n",
    "\n",
    "𝑦\n",
    "y is the dependent variable.\n",
    "𝑥\n",
    "x is the independent variable.\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept.\n",
    "𝛽\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is the slope.\n",
    "𝜖\n",
    "ϵ is the error term.\n",
    "Example:\n",
    "Suppose you want to predict the sales of ice cream based on temperature. You collect data on the temperature (independent variable) and ice cream sales (dependent variable) and fit a simple linear regression model.\n",
    "\n",
    "Sales\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "(\n",
    "Temperature\n",
    ")\n",
    "+\n",
    "𝜖\n",
    "Sales=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " (Temperature)+ϵ\n",
    "\n",
    "If the fitted model is:\n",
    "\n",
    "Sales\n",
    "=\n",
    "50\n",
    "+\n",
    "5\n",
    "(\n",
    "Temperature\n",
    ")\n",
    "Sales=50+5(Temperature)\n",
    "\n",
    "it means that for every one-degree increase in temperature, ice cream sales are expected to increase by 5 units.\n",
    "\n",
    "Multiple Linear Regression\n",
    "Definition:\n",
    "Multiple linear regression is an extension of simple linear regression that models the relationship between two or more independent variables (predictors) and a dependent variable (response). The relationship is modeled using a linear equation that includes multiple predictors:\n",
    "\n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "\n",
    "where:\n",
    "\n",
    "𝑦\n",
    "y is the dependent variable.\n",
    "𝑥\n",
    "1\n",
    ",\n",
    "𝑥\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "𝑥\n",
    "𝑛\n",
    "x \n",
    "1\n",
    "​\n",
    " ,x \n",
    "2\n",
    "​\n",
    " ,…,x \n",
    "n\n",
    "​\n",
    "  are the independent variables.\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept.\n",
    "𝛽\n",
    "1\n",
    ",\n",
    "𝛽\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "𝛽\n",
    "𝑛\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,…,β \n",
    "n\n",
    "​\n",
    "  are the coefficients.\n",
    "𝜖\n",
    "ϵ is the error term.\n",
    "Example:\n",
    "Suppose you want to predict the price of a house based on its size, number of bedrooms, and age. You collect data on these variables and fit a multiple linear regression model.\n",
    "\n",
    "Price\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "(\n",
    "Size\n",
    ")\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "(\n",
    "Bedrooms\n",
    ")\n",
    "+\n",
    "𝛽\n",
    "3\n",
    "(\n",
    "Age\n",
    ")\n",
    "+\n",
    "𝜖\n",
    "Price=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " (Size)+β \n",
    "2\n",
    "​\n",
    " (Bedrooms)+β \n",
    "3\n",
    "​\n",
    " (Age)+ϵ\n",
    "\n",
    "If the fitted model is:\n",
    "\n",
    "Price\n",
    "=\n",
    "20000\n",
    "+\n",
    "150\n",
    "(\n",
    "Size\n",
    ")\n",
    "+\n",
    "10000\n",
    "(\n",
    "Bedrooms\n",
    ")\n",
    "−\n",
    "5000\n",
    "(\n",
    "Age\n",
    ")\n",
    "Price=20000+150(Size)+10000(Bedrooms)−5000(Age)\n",
    "\n",
    "it means that, all else being equal:\n",
    "\n",
    "For every additional square foot in size, the house price increases by 150 units.\n",
    "For every additional bedroom, the house price increases by 10,000 units.\n",
    "For every additional year in age, the house price decreases by 5,000 units.\n",
    "Key Differences\n",
    "Number of Independent Variables:\n",
    "\n",
    "Simple Linear Regression: One independent variable.\n",
    "Multiple Linear Regression: Two or more independent variables.\n",
    "Complexity:\n",
    "\n",
    "Simple Linear Regression: Simpler to understand and interpret.\n",
    "Multiple Linear Regression: More complex due to the inclusion of multiple predictors and their potential interactions.\n",
    "Use Cases:\n",
    "\n",
    "Simple Linear Regression: Used when the relationship between a single predictor and response is of interest.\n",
    "Multiple Linear Regression: Used when the relationship between multiple predictors and the response needs to be modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f152da-d674-4c7f-8df8-d8ed5d101717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c4908ca-d661-44d3-8734-183504e9e55f",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c81d9-be6d-49d8-a328-76049e5771fb",
   "metadata": {},
   "source": [
    "Assumptions of Linear Regression\n",
    "Linear regression relies on several key assumptions that must be met for the results to be reliable and valid. Here are the main assumptions:\n",
    "\n",
    "Linearity:\n",
    "The relationship between the independent and dependent variables should be linear.\n",
    "\n",
    "Independence:\n",
    "Observations should be independent of each other. This means that the value of one observation does not influence the value of another.\n",
    "\n",
    "Homoscedasticity:\n",
    "The residuals (errors) should have constant variance at every level of the independent variables. In other words, the spread of the residuals should be roughly the same across all levels of the independent variables.\n",
    "\n",
    "Normality of Residuals:\n",
    "The residuals should be approximately normally distributed.\n",
    "\n",
    "No Multicollinearity (for Multiple Linear Regression):\n",
    "Independent variables should not be highly correlated with each other.\n",
    "\n",
    "Checking Assumptions\n",
    "Linearity:\n",
    "\n",
    "Scatterplot: Plot the dependent variable against each independent variable. Look for a linear relationship.\n",
    "Residuals vs. Fitted Values Plot: Check if the residuals are randomly dispersed around zero without any discernible pattern.\n",
    "Independence:\n",
    "\n",
    "Durbin-Watson Test: This statistical test detects the presence of autocorrelation in the residuals.\n",
    "Plot of Residuals: For time series data, plot the residuals against time to check for patterns.\n",
    "Homoscedasticity:\n",
    "\n",
    "Residuals vs. Fitted Values Plot: Plot the residuals against the predicted values. The spread of residuals should be consistent across all levels of the predicted values.\n",
    "Breusch-Pagan Test: A statistical test for heteroscedasticity.\n",
    "Normality of Residuals:\n",
    "\n",
    "Histogram of Residuals: Plot a histogram of the residuals and check for a roughly bell-shaped curve.\n",
    "Q-Q Plot: Plot the quantiles of the residuals against the quantiles of a normal distribution. Points should lie approximately on a straight line.\n",
    "Shapiro-Wilk Test: A statistical test for normality.\n",
    "No Multicollinearity:\n",
    "\n",
    "Variance Inflation Factor (VIF): Calculate VIF for each independent variable. A VIF value greater than 10 indicates high multicollinearity.\n",
    "Correlation Matrix: Check the correlation coefficients between independent variables. High correlation values (typically above 0.8) indicate multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0f498-ffd9-44a6-a0d7-d48b0a5c4566",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4acf17e-83fc-491c-8cfe-3ce09e0dd3a2",
   "metadata": {},
   "source": [
    "Interpretation of Slope and Intercept in a Linear Regression Model\n",
    "Slope (β₁): The slope represents the change in the dependent variable (y) for a one-unit increase in the independent variable (x). It indicates the strength and direction of the relationship between the independent and dependent variables.\n",
    "\n",
    "If the slope is positive, there is a positive relationship between the variables, meaning that as the independent variable increases, the dependent variable also increases.\n",
    "If the slope is negative, there is a negative relationship between the variables, meaning that as the independent variable increases, the dependent variable decreases.\n",
    "Intercept (β₀): The intercept is the value of the dependent variable when the independent variable is zero. It represents the point where the regression line crosses the y-axis.\n",
    "\n",
    "Example Using a Real-World Scenario\n",
    "Scenario:\n",
    "Suppose we want to predict the annual salary of employees based on their years of experience. We collect data on years of experience (independent variable) and annual salary (dependent variable) for a group of employees and fit a simple linear regression model.\n",
    "\n",
    "Linear Regression Equation:\n",
    "Salary\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "(\n",
    "Years of Experience\n",
    ")\n",
    "Salary=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " (Years of Experience)\n",
    "\n",
    "Assume we obtain the following regression equation after fitting the model:\n",
    "\n",
    "Salary\n",
    "=\n",
    "30000\n",
    "+\n",
    "5000\n",
    "(\n",
    "Years of Experience\n",
    ")\n",
    "Salary=30000+5000(Years of Experience)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Intercept (β₀ = 30000):\n",
    "The intercept value of 30,000 indicates that when an employee has zero years of experience, their predicted annual salary would be $30,000. This is the starting salary for an employee with no experience.\n",
    "\n",
    "Slope (β₁ = 5000):\n",
    "The slope value of 5,000 indicates that for each additional year of experience, the annual salary of the employee is expected to increase by $5,000. This shows a positive relationship between years of experience and annual salary.\n",
    "\n",
    "Detailed Example:\n",
    "Employee with 0 Years of Experience:\n",
    "\n",
    "Predicted Salary = 30,000 + 5,000(0) = $30,000\n",
    "Interpretation: An employee with no experience is predicted to earn $30,000 annually.\n",
    "Employee with 5 Years of Experience:\n",
    "\n",
    "Predicted Salary = 30,000 + 5,000(5) = $55,000\n",
    "Interpretation: An employee with 5 years of experience is predicted to earn $55,000 annually.\n",
    "Employee with 10 Years of Experience:\n",
    "\n",
    "Predicted Salary = 30,000 + 5,000(10) = $80,000\n",
    "Interpretation: An employee with 10 years of experience is predicted to earn $80,000 annually.\n",
    "General Insights:\n",
    "Understanding the Model:\n",
    "\n",
    "The intercept provides a baseline value, helping to understand the starting point of the dependent variable.\n",
    "The slope quantifies the effect of the independent variable on the dependent variable, showing how much the dependent variable changes with a one-unit change in the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4120543-76f4-4498-b33b-965cafb3637b",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721e687e-8e64-4c62-9647-c0e5ecd4c47b",
   "metadata": {},
   "source": [
    "Concept of Gradient Descent\n",
    "Gradient Descent: Gradient descent is an optimization algorithm used to minimize the cost function in machine learning and deep learning models. The cost function measures how well a model's predictions match the actual data. Gradient descent iteratively adjusts the model's parameters (weights and biases) to find the optimal values that minimize the cost function.\n",
    "\n",
    "How Gradient Descent Works\n",
    "Initialization: Start with an initial set of parameters (often chosen randomly).\n",
    "Compute Gradient: Calculate the gradient (partial derivatives) of the cost function with respect to each parameter. The gradient indicates the direction and rate of the steepest increase in the cost function.\n",
    "Update Parameters: Adjust the parameters in the opposite direction of the gradient by a factor called the learning rate. This step moves the parameters towards the minimum of the cost function.\n",
    "Iterate: Repeat the process until the cost function converges to a minimum value or until a predefined number of iterations is reached.\n",
    "The update rule for the parameters \n",
    "𝜃\n",
    "θ is:\n",
    "\n",
    "𝜃\n",
    "←\n",
    "𝜃\n",
    "−\n",
    "𝛼\n",
    "⋅\n",
    "∇\n",
    "𝜃\n",
    "𝐽\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "θ←θ−α⋅∇ \n",
    "θ\n",
    "​\n",
    " J(θ)\n",
    "\n",
    "where:\n",
    "\n",
    "𝜃\n",
    "θ represents the parameters (weights and biases).\n",
    "𝛼\n",
    "α is the learning rate.\n",
    "∇\n",
    "𝜃\n",
    "𝐽\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "∇ \n",
    "θ\n",
    "​\n",
    " J(θ) is the gradient of the cost function \n",
    "𝐽\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "J(θ) with respect to \n",
    "𝜃\n",
    "θ.\n",
    "Types of Gradient Descent\n",
    "Batch Gradient Descent: Uses the entire dataset to compute the gradient at each step. It is computationally expensive for large datasets but provides a smooth convergence.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): Uses one training example at each iteration to compute the gradient. It is faster and can handle large datasets, but the path to convergence is noisier.\n",
    "\n",
    "Mini-Batch Gradient Descent: Uses a subset (mini-batch) of the dataset to compute the gradient at each step. It balances the trade-off between batch gradient descent and SGD, providing faster convergence with less noise.\n",
    "\n",
    "Use of Gradient Descent in Machine Learning\n",
    "Gradient descent is used extensively in training various machine learning models, including linear regression, logistic regression, neural networks, and support vector machines. Here’s how it is applied in different contexts:\n",
    "\n",
    "Linear Regression:\n",
    "\n",
    "Objective: Minimize the mean squared error (MSE) between the predicted values and actual values.\n",
    "Cost Function: \n",
    "𝐽\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "=\n",
    "1\n",
    "2\n",
    "𝑚\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑚\n",
    "(\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    "(\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "−\n",
    "𝑦\n",
    "(\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "2\n",
    "J(θ)= \n",
    "2m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " (h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " )−y \n",
    "(i)\n",
    " ) \n",
    "2\n",
    " \n",
    "Gradient Descent Update: Adjust the weights and bias to minimize the MSE.\n",
    "Logistic Regression:\n",
    "\n",
    "Objective: Minimize the binary cross-entropy loss.\n",
    "Cost Function: \n",
    "𝐽\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "𝑚\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑚\n",
    "[\n",
    "𝑦\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    "(\n",
    "𝑖\n",
    ")\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑦\n",
    "(\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    "(\n",
    "𝑖\n",
    ")\n",
    ")\n",
    ")\n",
    "]\n",
    "J(θ)=− \n",
    "m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " [y \n",
    "(i)\n",
    " log(h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))+(1−y \n",
    "(i)\n",
    " )log(1−h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))]\n",
    "Gradient Descent Update: Adjust the weights and bias to minimize the cross-entropy loss.\n",
    "Neural Networks:\n",
    "\n",
    "Objective: Minimize the loss function (e.g., cross-entropy loss for classification, MSE for regression).\n",
    "Backpropagation: Gradient descent is used in conjunction with backpropagation to update the weights and biases of the network.\n",
    "Example: Linear Regression with Gradient Descent\n",
    "Suppose we are training a linear regression model to predict house prices based on the size of the house. The goal is to minimize the cost function \n",
    "𝐽\n",
    "(\n",
    "𝜃\n",
    ")\n",
    "J(θ), which measures the difference between the predicted and actual house prices.\n",
    "\n",
    "Initialization:\n",
    "\n",
    "Start with random values for the parameters (weights and bias).\n",
    "Compute Gradient:\n",
    "\n",
    "Calculate the gradient of the cost function with respect to each parameter.\n",
    "Update Parameters:\n",
    "\n",
    "Adjust the parameters using the gradient and the learning rate.\n",
    "Iterate:\n",
    "\n",
    "Repeat the process until the cost function converges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26e88e-59ae-4b96-af69-b7da8db7d13a",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e054e9-1722-4d95-8cd8-3b87a7de79f5",
   "metadata": {},
   "source": [
    "Multiple Linear Regression Model\n",
    "Multiple Linear Regression is an extension of simple linear regression that models the relationship between two or more independent variables (predictors) and a dependent variable (response). The model can be represented by the equation:\n",
    "\n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "\n",
    "where:\n",
    "\n",
    "𝑦\n",
    "y is the dependent variable.\n",
    "𝑥\n",
    "1\n",
    ",\n",
    "𝑥\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "𝑥\n",
    "𝑛\n",
    "x \n",
    "1\n",
    "​\n",
    " ,x \n",
    "2\n",
    "​\n",
    " ,…,x \n",
    "n\n",
    "​\n",
    "  are the independent variables.\n",
    "𝛽\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    "  is the intercept.\n",
    "𝛽\n",
    "1\n",
    ",\n",
    "𝛽\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "𝛽\n",
    "𝑛\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,…,β \n",
    "n\n",
    "​\n",
    "  are the coefficients.\n",
    "𝜖\n",
    "ϵ is the error term.\n",
    "How It Works\n",
    "Data Collection: Gather data on the dependent variable and multiple independent variables.\n",
    "Model Fitting: Use statistical software to estimate the coefficients (\n",
    "𝛽\n",
    "0\n",
    ",\n",
    "𝛽\n",
    "1\n",
    ",\n",
    "…\n",
    ",\n",
    "𝛽\n",
    "𝑛\n",
    "β \n",
    "0\n",
    "​\n",
    " ,β \n",
    "1\n",
    "​\n",
    " ,…,β \n",
    "n\n",
    "​\n",
    " ) that minimize the cost function (e.g., mean squared error).\n",
    "Prediction: Use the fitted model to predict the dependent variable based on new values of the independent variables.\n",
    "Interpretation: Analyze the estimated coefficients to understand the relationship between the independent variables and the dependent variable.\n",
    "Example Scenario\n",
    "Suppose we want to predict the price of a house based on its size (in square feet), the number of bedrooms, and the age of the house. We collect data on these variables and fit a multiple linear regression model.\n",
    "\n",
    "Multiple Linear Regression Equation:\n",
    "Price\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "(\n",
    "Size\n",
    ")\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "(\n",
    "Bedrooms\n",
    ")\n",
    "+\n",
    "𝛽\n",
    "3\n",
    "(\n",
    "Age\n",
    ")\n",
    "+\n",
    "𝜖\n",
    "Price=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " (Size)+β \n",
    "2\n",
    "​\n",
    " (Bedrooms)+β \n",
    "3\n",
    "​\n",
    " (Age)+ϵ\n",
    "\n",
    "Assume we obtain the following regression equation after fitting the model:\n",
    "\n",
    "Price\n",
    "=\n",
    "50\n",
    ",\n",
    "000\n",
    "+\n",
    "150\n",
    "(\n",
    "Size\n",
    ")\n",
    "+\n",
    "10\n",
    ",\n",
    "000\n",
    "(\n",
    "Bedrooms\n",
    ")\n",
    "−\n",
    "2\n",
    ",\n",
    "000\n",
    "(\n",
    "Age\n",
    ")\n",
    "Price=50,000+150(Size)+10,000(Bedrooms)−2,000(Age)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Intercept (\n",
    "𝛽\n",
    "0\n",
    "=\n",
    "50\n",
    ",\n",
    "000\n",
    "β \n",
    "0\n",
    "​\n",
    " =50,000): When all independent variables are zero, the predicted price of the house is $50,000.\n",
    "Coefficient for Size (\n",
    "𝛽\n",
    "1\n",
    "=\n",
    "150\n",
    "β \n",
    "1\n",
    "​\n",
    " =150): For each additional square foot in size, the house price is expected to increase by $150, holding all other variables constant.\n",
    "Coefficient for Bedrooms (\n",
    "𝛽\n",
    "2\n",
    "=\n",
    "10\n",
    ",\n",
    "000\n",
    "β \n",
    "2\n",
    "​\n",
    " =10,000): For each additional bedroom, the house price is expected to increase by $10,000, holding all other variables constant.\n",
    "Coefficient for Age (\n",
    "𝛽\n",
    "3\n",
    "=\n",
    "−\n",
    "2\n",
    ",\n",
    "000\n",
    "β \n",
    "3\n",
    "​\n",
    " =−2,000): For each additional year in age, the house price is expected to decrease by $2,000, holding all other variables constant.\n",
    "Differences Between Multiple Linear Regression and Simple Linear Regression\n",
    "Number of Independent Variables:\n",
    "\n",
    "Simple Linear Regression: Models the relationship between one independent variable and the dependent variable.\n",
    "Multiple Linear Regression: Models the relationship between two or more independent variables and the dependent variable.\n",
    "Complexity:\n",
    "\n",
    "Simple Linear Regression: Easier to visualize and interpret due to the single predictor.\n",
    "Multiple Linear Regression: More complex due to the inclusion of multiple predictors, requiring more sophisticated analysis and interpretation.\n",
    "Equation:\n",
    "\n",
    "Simple Linear Regression: \n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+ϵ\n",
    "Multiple Linear Regression: \n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "…\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "Interpretation of Coefficients:\n",
    "\n",
    "Simple Linear Regression: The coefficient represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "Multiple Linear Regression: Each coefficient represents the change in the dependent variable for a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
    "Assumptions:\n",
    "\n",
    "Both models share similar assumptions (linearity, independence, homoscedasticity, normality of residuals), but multiple linear regression also requires checking for multicollinearity among the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3628d-e1d2-4318-9fee-1a4f1f4322eb",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01205ab7-cf6b-436f-8592-f041a37ab55c",
   "metadata": {},
   "source": [
    "Concept of Multicollinearity in Multiple Linear Regression\n",
    "Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly correlated with each other. This high correlation among independent variables can cause problems because it becomes difficult to isolate the individual effect of each predictor on the dependent variable. As a result, the estimates of the regression coefficients can become unstable and inflated, leading to less reliable statistical inferences.\n",
    "\n",
    "Problems Caused by Multicollinearity\n",
    "Inflated Standard Errors: When multicollinearity is present, the standard errors of the coefficients become large, making the estimates less precise.\n",
    "Unstable Estimates: Small changes in the data can lead to large changes in the estimates of the coefficients.\n",
    "Reduced Significance: The statistical significance of individual predictors may be underestimated, making it hard to determine which predictors are important.\n",
    "Detecting Multicollinearity\n",
    "Correlation Matrix:\n",
    "\n",
    "Calculate the pairwise correlation coefficients between the independent variables. High correlation values (typically above 0.8 or 0.9) indicate multicollinearity.\n",
    "Variance Inflation Factor (VIF):\n",
    "\n",
    "The VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity. It is calculated as:\n",
    "VIF\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "1\n",
    "−\n",
    "𝑅\n",
    "𝑖\n",
    "2\n",
    "VIF \n",
    "i\n",
    "​\n",
    " = \n",
    "1−R \n",
    "i\n",
    "2\n",
    "​\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "where \n",
    "𝑅\n",
    "𝑖\n",
    "2\n",
    "R \n",
    "i\n",
    "2\n",
    "​\n",
    "  is the coefficient of determination of the regression of the \n",
    "𝑖\n",
    "i-th independent variable on all other independent variables.\n",
    "A VIF value greater than 10 is often considered indicative of high multicollinearity.\n",
    "Tolerance:\n",
    "\n",
    "Tolerance is the reciprocal of VIF:\n",
    "Tolerance\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "VIF\n",
    "𝑖\n",
    "Tolerance \n",
    "i\n",
    "​\n",
    " = \n",
    "VIF \n",
    "i\n",
    "​\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "A tolerance value less than 0.1 indicates high multicollinearity.\n",
    "Eigenvalues and Condition Index:\n",
    "\n",
    "Compute the eigenvalues and condition index of the matrix of independent variables. A condition index above 30 indicates potential multicollinearity issues.\n",
    "Addressing Multicollinearity\n",
    "Remove Highly Correlated Predictors:\n",
    "\n",
    "Identify and remove one of the highly correlated variables from the model to reduce multicollinearity.\n",
    "Combine Variables:\n",
    "\n",
    "Combine correlated variables into a single predictor through techniques such as principal component analysis (PCA) or by creating an index.\n",
    "Regularization Techniques:\n",
    "\n",
    "Use regularization methods like Ridge Regression (L2 regularization) or Lasso Regression (L1 regularization) which add a penalty to the coefficients, thereby reducing the impact of multicollinearity.\n",
    "Centering the Data:\n",
    "\n",
    "Centering the predictors by subtracting the mean can sometimes reduce multicollinearity, especially in polynomial regression models.\n",
    "Collect More Data:\n",
    "\n",
    "Increasing the sample size can sometimes help in reducing multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f834d-8383-4454-9297-f9d5798ad7d1",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d3d64-3672-46ae-a60e-dff29061310a",
   "metadata": {},
   "source": [
    "Polynomial Regression Model\n",
    "Polynomial Regression is a type of regression analysis where the relationship between the independent variable \n",
    "𝑥\n",
    "x and the dependent variable \n",
    "𝑦\n",
    "y is modeled as an \n",
    "𝑛\n",
    "n-th degree polynomial. Unlike linear regression, which fits a straight line to the data, polynomial regression fits a curve.\n",
    "\n",
    "Polynomial Regression Equation\n",
    "The polynomial regression equation of degree \n",
    "𝑛\n",
    "n is given by:\n",
    "\n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "𝛽\n",
    "3\n",
    "𝑥\n",
    "3\n",
    "+\n",
    "…\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    " +β \n",
    "3\n",
    "​\n",
    " x \n",
    "3\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    " +ϵ\n",
    "\n",
    "where:\n",
    "\n",
    "𝑦\n",
    "y is the dependent variable.\n",
    "𝑥\n",
    "x is the independent variable.\n",
    "𝛽\n",
    "0\n",
    ",\n",
    "𝛽\n",
    "1\n",
    ",\n",
    "𝛽\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "𝛽\n",
    "𝑛\n",
    "β \n",
    "0\n",
    "​\n",
    " ,β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,…,β \n",
    "n\n",
    "​\n",
    "  are the coefficients.\n",
    "𝜖\n",
    "ϵ is the error term.\n",
    "How Polynomial Regression Works\n",
    "Data Collection: Gather data on the dependent variable and the independent variable(s).\n",
    "Transformation: Transform the independent variable \n",
    "𝑥\n",
    "x into polynomial terms \n",
    "𝑥\n",
    ",\n",
    "𝑥\n",
    "2\n",
    ",\n",
    "𝑥\n",
    "3\n",
    ",\n",
    "…\n",
    ",\n",
    "𝑥\n",
    "𝑛\n",
    "x,x \n",
    "2\n",
    " ,x \n",
    "3\n",
    " ,…,x \n",
    "n\n",
    " .\n",
    "Model Fitting: Use a method (like least squares) to estimate the coefficients that minimize the cost function (e.g., mean squared error).\n",
    "Prediction: Use the fitted polynomial model to predict the dependent variable based on new values of the independent variable.\n",
    "Interpretation: Analyze the estimated coefficients to understand the relationship between the independent variable and the dependent variable.\n",
    "Example Scenario\n",
    "Suppose we want to predict the price of a house based on its size. We collect data on house prices and sizes and fit a polynomial regression model.\n",
    "\n",
    "Polynomial Regression Equation:\n",
    "Assume we decide to use a quadratic (2nd degree) polynomial regression model:\n",
    "\n",
    "Price\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "(\n",
    "Size\n",
    ")\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "(\n",
    "Size\n",
    "2\n",
    ")\n",
    "+\n",
    "𝜖\n",
    "Price=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " (Size)+β \n",
    "2\n",
    "​\n",
    " (Size \n",
    "2\n",
    " )+ϵ\n",
    "\n",
    "After fitting the model, we obtain the following regression equation:\n",
    "\n",
    "Price\n",
    "=\n",
    "50000\n",
    "+\n",
    "200\n",
    "(\n",
    "Size\n",
    ")\n",
    "−\n",
    "0.1\n",
    "(\n",
    "Size\n",
    "2\n",
    ")\n",
    "Price=50000+200(Size)−0.1(Size \n",
    "2\n",
    " )\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Intercept (\n",
    "𝛽\n",
    "0\n",
    "=\n",
    "50000\n",
    "β \n",
    "0\n",
    "​\n",
    " =50000): When the size is zero, the predicted price of the house is $50,000.\n",
    "Coefficient for Size (\n",
    "𝛽\n",
    "1\n",
    "=\n",
    "200\n",
    "β \n",
    "1\n",
    "​\n",
    " =200): For each additional square foot, the house price increases by $200 initially, holding the quadratic term constant.\n",
    "Coefficient for Size Squared (\n",
    "𝛽\n",
    "2\n",
    "=\n",
    "−\n",
    "0.1\n",
    "β \n",
    "2\n",
    "​\n",
    " =−0.1): The negative coefficient indicates that the rate of increase in price decreases as the size increases, suggesting a nonlinear relationship.\n",
    "Differences Between Polynomial Regression and Linear Regression\n",
    "Model Complexity:\n",
    "\n",
    "Linear Regression: Models the relationship between the dependent and independent variable as a straight line.\n",
    "Polynomial Regression: Models the relationship as a polynomial curve, allowing for more complex relationships.\n",
    "Equation:\n",
    "\n",
    "Linear Regression: \n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+ϵ\n",
    "Polynomial Regression: \n",
    "𝑦\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "𝛽\n",
    "3\n",
    "𝑥\n",
    "3\n",
    "+\n",
    "…\n",
    "+\n",
    "𝛽\n",
    "𝑛\n",
    "𝑥\n",
    "𝑛\n",
    "+\n",
    "𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    " +β \n",
    "3\n",
    "​\n",
    " x \n",
    "3\n",
    " +…+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    " +ϵ\n",
    "Flexibility:\n",
    "\n",
    "Linear Regression: Limited to linear relationships.\n",
    "Polynomial Regression: Can capture more complex, nonlinear relationships.\n",
    "Bias-Variance Tradeoff:\n",
    "\n",
    "Linear Regression: Generally has lower variance but higher bias if the true relationship is nonlinear.\n",
    "Polynomial Regression: Can reduce bias for nonlinear relationships but may increase variance, especially with high-degree polynomials, leading to overfitting.\n",
    "Visual Representation:\n",
    "\n",
    "Linear Regression: Fits a straight line through the data points.\n",
    "Polynomial Regression: Fits a curved line (polynomial) through the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bcc356-f29e-47c8-a7a1-c47892dad0b2",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b3c69-ff28-4a24-ba0a-b3bc0e9a7d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
